<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
</head>
<body>

<h1>Satellite Imagery–Based Property Valuation</h1>

<h2>Overview</h2>
<p>
This project implements a <b>multimodal regression pipeline</b> to predict property prices by combining structured housing attributes with satellite imagery. Satellite images are fetched using geographic coordinates and processed using a pretrained convolutional neural network (CNN) to extract visual features capturing neighborhood and environmental context.
</p>

<p>
The visual embeddings are fused with tabular features to train a regression model for property valuation. Model interpretability is demonstrated using Grad-CAM visualizations.
</p>

<hr>

<h2>Project Structure</h2>

<pre>
satellite-property-valuation/
│
├── data/
│   ├── raw/
│   ├── processed/
│   └── images/
│
├── notebooks/
│   ├── eda.ipynb
│   ├── preprocessing.ipynb
    ├── cnn_feature_extraction.ipynb
│   ├── image_embeddings.ipynb
│   ├── multimodal_training.ipynb
│   └── gradcam.ipynb
    
│
├── src/
│   └── data_fetcher.py
│
├── submission.csv
├── README.md
└── requirements.txt
</pre>

<hr>

<h2>Data Availability</h2>

<p>
The <code>data/</code> directory is not included in this repository. This is intentional due to the large size of satellite images and the use of
external APIs for image acquisition.
</p>

<p>
All data used in this project can be regenerated by following the provided notebooks and scripts:
</p>

<ul>
    <li>Raw tabular datasets should be placed in <code>data/raw/</code></li>
    <li>Satellite images are downloaded programmatically using <code>src/data_fetcher.py</code></li>
    <li>Processed tabular data and image embeddings are generated via the preprocessing and embedding notebooks</li>
</ul>

<p>
This design ensures reproducibility while keeping the repository lightweight and API-compliant.
</p>


<h2>Setup Instructions</h2>

<h3>1. Create and activate virtual environment</h3>

<pre>
python -m venv houseval
</pre>

<p><b>Windows:</b></p>
<pre>
.\houseval\Scripts\activate
</pre>

<h3>2. Install dependencies</h3>

<pre>
pip install -r requirements.txt
</pre>

<h3>3. Set up Mapbox API key</h3>

<p>
Satellite imagery is fetched using the Mapbox Static Images API. Set the API key as an environment variable or inside a <code>.env</code> file:
</p>

<pre>
MAPBOX_API_KEY=your_mapbox_api_key_here
</pre>

<hr>

<h2>Running the Project</h2>

<h3>Step 1: Exploratory Data Analysis</h3>
<p>
Run <code>notebooks/eda.ipynb</code> to explore the data distribution, feature relationships, and pricing trends.
</p>

<h3>Step 2: Data Preprocessing</h3>
<p>
Run <code>notebooks/preprocessing.ipynb</code> to clean data, select features, and generate processed train/test tabular datasets.
</p>

<h3>Step 3: Download Satellite Images</h3>

<pre>
python src/data_fetcher.py
</pre>

<p>
This script downloads satellite images using latitude and longitude coordinates and skips images that already exist.
</p>

<h3>Step 4: Generate Image Embeddings</h3>
<p>
Run <code>notebooks/image_embeddings.ipynb</code> to extract visual embeddings from satellite images using a pretrained ResNet18 model.
</p>

<h3>Step 5: Multimodal Model Training</h3>
<p>
Run <code>notebooks/multimodal_training.ipynb</code> to train:
</p>
<ul>
    <li>Tabular-only regression model</li>
    <li>Multimodal regression model (tabular + image embeddings)</li>
</ul>

<p>
The notebook evaluates performance using RMSE and R² and generates <code>submission.csv</code>.
</p>

<h3>Step 6: Model Explainability</h3>
<p>
Run <code>notebooks/gradcam.ipynb</code> to generate Grad-CAM visualizations highlighting influential regions in satellite images.
</p>

<hr>

<h2>Output</h2>

<ul>
    <li><b>submission.csv</b> with format:
        <pre>id, predicted_price</pre>
    </li>
    <li>Grad-CAM visualizations for explainability</li>
</ul>

<hr>

<h2>Notes</h2>

<ul>
    <li>The house identifier in the raw data is not unique and is therefore not used as a submission key.</li>
    <li>Test predictions are indexed using the original row order.</li>
    <li>Grad-CAM is applied to the CNN feature extractor, not the regression model.</li>
</ul>

<hr>

<h2>Tech Stack</h2>

<ul>
    <li>Python, Pandas, NumPy</li>
    <li>PyTorch, Torchvision</li>
    <li>Scikit-learn</li>
    <li>OpenCV, PIL</li>
    <li>Mapbox Static Images API</li>
</ul>

<hr>

<h2>Author</h2>
<p>
<b>Arpita Jain</b><br>
Multimodal Machine Learning Project
</p>

</body>
</html>
