{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc20cfad",
   "metadata": {},
   "source": [
    "# CNN Feature Extraction (Satellite Images)\n",
    "\n",
    "This notebook defines a pretrained CNN-based feature extractor for satellite images. The model is used to convert images into fixed-size embeddings for downstream multimodal regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f97365d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9275d66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a07115a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>sqft_living</th>\n",
       "      <th>sqft_lot</th>\n",
       "      <th>floors</th>\n",
       "      <th>waterfront</th>\n",
       "      <th>view</th>\n",
       "      <th>condition</th>\n",
       "      <th>grade</th>\n",
       "      <th>sqft_living15</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>log_price</th>\n",
       "      <th>image_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.677402</td>\n",
       "      <td>0.178963</td>\n",
       "      <td>-0.290276</td>\n",
       "      <td>-0.144952</td>\n",
       "      <td>0.922943</td>\n",
       "      <td>-0.083788</td>\n",
       "      <td>-0.306964</td>\n",
       "      <td>-0.626000</td>\n",
       "      <td>-0.557611</td>\n",
       "      <td>-0.473911</td>\n",
       "      <td>-0.900034</td>\n",
       "      <td>0.192759</td>\n",
       "      <td>12.501142</td>\n",
       "      <td>../data/images/train/9117000170_0.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.394132</td>\n",
       "      <td>0.505667</td>\n",
       "      <td>-0.521813</td>\n",
       "      <td>-0.311135</td>\n",
       "      <td>0.922943</td>\n",
       "      <td>-0.083788</td>\n",
       "      <td>-0.306964</td>\n",
       "      <td>0.908842</td>\n",
       "      <td>-0.557611</td>\n",
       "      <td>-0.385919</td>\n",
       "      <td>-1.137139</td>\n",
       "      <td>0.192759</td>\n",
       "      <td>12.409018</td>\n",
       "      <td>../data/images/train/6700390210_1.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.677402</td>\n",
       "      <td>0.505667</td>\n",
       "      <td>-0.389506</td>\n",
       "      <td>-0.160457</td>\n",
       "      <td>0.922943</td>\n",
       "      <td>-0.083788</td>\n",
       "      <td>-0.306964</td>\n",
       "      <td>-0.626000</td>\n",
       "      <td>0.296350</td>\n",
       "      <td>-0.165941</td>\n",
       "      <td>-2.098571</td>\n",
       "      <td>-0.706669</td>\n",
       "      <td>12.206078</td>\n",
       "      <td>../data/images/train/7212660540_2.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.465666</td>\n",
       "      <td>0.178963</td>\n",
       "      <td>-0.918734</td>\n",
       "      <td>-0.364787</td>\n",
       "      <td>0.922943</td>\n",
       "      <td>-0.083788</td>\n",
       "      <td>-0.306964</td>\n",
       "      <td>-0.626000</td>\n",
       "      <td>-0.557611</td>\n",
       "      <td>-1.089851</td>\n",
       "      <td>-0.206791</td>\n",
       "      <td>1.006527</td>\n",
       "      <td>12.772806</td>\n",
       "      <td>../data/images/train/8562780200_3.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.394132</td>\n",
       "      <td>-0.147741</td>\n",
       "      <td>-0.874632</td>\n",
       "      <td>-0.038936</td>\n",
       "      <td>-0.918626</td>\n",
       "      <td>-0.083788</td>\n",
       "      <td>-0.306964</td>\n",
       "      <td>-0.626000</td>\n",
       "      <td>-0.557611</td>\n",
       "      <td>-0.576568</td>\n",
       "      <td>-1.367738</td>\n",
       "      <td>0.999388</td>\n",
       "      <td>12.354497</td>\n",
       "      <td>../data/images/train/7760400350_4.png</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bedrooms  bathrooms  sqft_living  sqft_lot    floors  waterfront      view  \\\n",
       "0  0.677402   0.178963    -0.290276 -0.144952  0.922943   -0.083788 -0.306964   \n",
       "1 -0.394132   0.505667    -0.521813 -0.311135  0.922943   -0.083788 -0.306964   \n",
       "2  0.677402   0.505667    -0.389506 -0.160457  0.922943   -0.083788 -0.306964   \n",
       "3 -1.465666   0.178963    -0.918734 -0.364787  0.922943   -0.083788 -0.306964   \n",
       "4 -0.394132  -0.147741    -0.874632 -0.038936 -0.918626   -0.083788 -0.306964   \n",
       "\n",
       "   condition     grade  sqft_living15       lat      long  log_price  \\\n",
       "0  -0.626000 -0.557611      -0.473911 -0.900034  0.192759  12.501142   \n",
       "1   0.908842 -0.557611      -0.385919 -1.137139  0.192759  12.409018   \n",
       "2  -0.626000  0.296350      -0.165941 -2.098571 -0.706669  12.206078   \n",
       "3  -0.626000 -0.557611      -1.089851 -0.206791  1.006527  12.772806   \n",
       "4  -0.626000 -0.557611      -0.576568 -1.367738  0.999388  12.354497   \n",
       "\n",
       "                              image_path  \n",
       "0  ../data/images/train/9117000170_0.png  \n",
       "1  ../data/images/train/6700390210_1.png  \n",
       "2  ../data/images/train/7212660540_2.png  \n",
       "3  ../data/images/train/8562780200_3.png  \n",
       "4  ../data/images/train/7760400350_4.png  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"../data/processed/train_tabular.csv\")\n",
    "test_df  = pd.read_csv(\"../data/processed/test_tabular.csv\")\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ffaa8696",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "647d5a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\arpit\\Desktop\\cdc project\\houseval\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\arpit\\Desktop\\cdc project\\houseval\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to C:\\Users\\arpit/.cache\\torch\\hub\\checkpoints\\resnet18-f37072fd.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 44.7M/44.7M [00:03<00:00, 13.2MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet = models.resnet18(pretrained=True)\n",
    "resnet = resnet.to(device)\n",
    "resnet.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a698c438",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = torch.nn.Sequential(\n",
    "    *list(resnet.children())[:-1]\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81f44312",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_path = train_df.loc[0, \"image_path\"]\n",
    "\n",
    "assert os.path.exists(img_path), \"Image path does not exist!\"\n",
    "\n",
    "img = Image.open(img_path).convert(\"RGB\")\n",
    "img_tensor = image_transforms(img).unsqueeze(0).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    features = feature_extractor(img_tensor)\n",
    "\n",
    "features = features.view(-1)\n",
    "features.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "houseval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
